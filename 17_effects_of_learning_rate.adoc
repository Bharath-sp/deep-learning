= Effects of Learning Rate =
:doctype: book
:stem: latexmath
:eqnums:
:toc:

Learning Rate affects the convergence.

== Convergence of Convex Functions ==
Quadratic functions are convex functions. A real-valued quadratic function of one variable is

[stem]
++++
L(w) = \frac{1}{2} Cw^2 + dw + e
++++

The second-order approximation of stem:[L] within the neighborhood of a point stem:[w^*] can be given by the Taylor series as

[stem]
++++
L(w) = L(w^*) + (w-w^*) \frac{\partial L(w^*)}{\partial w} + \frac{1}{2} (w-w^*)^2 \frac{\partial L^2(w^*)}{\partial w^2}
++++

Note stem:[w^*] is a fixed point, therefore stem:[\frac{\partial L(w^*)}{\partial w} = Cw^*+d] and stem:[\frac{\partial L^2(w^*)}{\partial w^2}=C] are constants.

If the loss function attains a local minima at stem:[w] (in the neighborhood of stem:[w^*]), then the derivative at that stem:[w] must be zero.

[stem]
++++
\begin{align*}
\frac{\partial L(w)}{\partial w} & = 0 \\
\frac{\partial L(w^*)}{\partial w} + (w_{\text{opt}} -w^*) \frac{\partial L^2(w^*)}{\partial w^2} & = 0 \\
\\
L'(w^*) + (w_{\text{opt}}-w^*) L''(w^*) & = 0 \\
(w_{\text{opt}}-w^*) L''(w^*) & = - L'(w^*) \\
w_{\text{opt}} & = w^* - \frac{L'(w^*)}{L''(w^*)}
\end{align*}
++++

Now assume we are at any fixed point stem:[w^*] (it can be anywhere in the domain of the function). We can reach the optimal stem:[w] (the local minima) using the above equation in a single step.

TIP: Convex functions has only one local minima and it is also the global minima.

The equation is similar to the gradient descent update equation.

[stem]
++++
\begin{align*}
w_{\text{opt}} & = w^* - \frac{1}{L''(w^*)} \cdot L'(w^*) \\
w_{\text{opt}} & = w^* - \frac{1}{C} \cdot L'(w^*)
\end{align*}
++++

Therefore, for a quadratic function stem:[L(w) = \frac{1}{2} Cw^2 + dw + e], if we take the step size stem:[\eta = \frac{1}{C}], the gradient descent algorithm will converge to the optimal value in a single update. This is the optimal step.

* If stem:[\eta < \frac{1}{C}], we will converge slowly through multiple steps.
* If stem:[\eta = \frac{2}{C}], we will remain oscillating without convergence. The quadratic functions are symmetric about an axis going through the local minima. The minima will be at a distance of stem:[\frac{1}{C} \cdot L'(w^*)] from any given point stem:[w^*]. But stem:[\eta = \frac{2}{C}], we keep moving a distance of stem:[2 * \frac{1}{C} \cdot L'(w^*)] from stem:[w^*]. This results in oscillation (no convergence and no divergence).
* If stem:[\eta > \frac{2}{C}], we will diverge. At each step, the loss function value will be increasing.
* If stem:[\frac{1}{C} < \eta < \frac{2}{C}], we will oscillate, but converge later.

image::.\images\issues_with_gd_01.png[align='center', 800, 500]

Hence stem:[\eta] is an important parameter and it should be chosen carefully. Based on its value, we will either converge, diverge, or oscillate around the local minima.

Not all loss functions are quadratic. In the case of generic functions, we can perform a quadratic approximation. Say we have a function (the blue curve) which is not quadratic. We can approximate the function within the neighborhood of a point stem:[w_0] by the Taylor series.

The green curve is the quadratic approximation of the loss function at stem:[w_0]. Corresponding to this quadratic approximation, we take a learning rate of stem:[\eta = \frac{1}{C}] and update stem:[w] to the lowest point of this green quadratic curve. At stem:[w_1], we repeat the same process.

image::.\images\quad_approx.png[align='center', 600, 500]

This method is known as Newton's method.

=== Multivariate Functions ===
The loss function can be a function of more than one variable. The quadratic multivariate function is given by

[stem]
++++
f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^\top \mathbf{A} \mathbf{x} + \mathbf{x}^\top \mathbf{b} + c
++++

The loss function may have different curvatures (double derivative) in each direction, i.e., the function may increase fast in one direction compared to the other direction. Thus the optimal learning rate will be different for different directions.

On expanding the above function for two variable case:

[stem]
++++
f(\mathbf{x}) = \frac{1}{2} a_{11} x_1^2 + \frac{1}{2} a_{22} x_2^2 + \frac{1}{2} (a_{12} + a_{21}) x_1x_2 + b_1x_1 + b_2x_2 + C
++++

stem:[a_{11}] is the curvature (double derivative) with respect to stem:[x_1] and stem:[a_{22}] is the curvature with respect to stem:[x_2]. Then the learning rate has to be stem:[< \frac{2}{a_{11}}] in the stem:[x_1] direction and stem:[< \frac{2}{a_{22}}] in the stem:[x_2] direction for the optimal step. If the learning rate is stem:[> \frac{2}{a_{kk}}] in any of the directions, we will diverge in that direction.

====
In gradient descent, the learning rate must be smaller than the twice the smallest optimal learning rate stem:[\eta < 2 \cdot \eta_{\text{min}}]. Else, it may diverge.

This makes the convergence slow and oscillate in some directions.
====

The loss reduces as per the learning rate:

image::.\images\issues_with_gd_02.png[align='center', 400, 400]

It is also a good idea to gradually reduce the learning rate while iterating and as we go towards the target point. We can reduce the learning rate after regular intervals or reduce it continuously (linearly, exponentially, etc.). For example

* After every 30 epochs, stem:[\eta^* = 0.1 * \eta], or 
* stem:[\eta_t = \eta_0 (1-\frac{\alpha}{100})^t], where stem:[\eta_0] can be set to a large value and stem:[t] is the iteration number.

