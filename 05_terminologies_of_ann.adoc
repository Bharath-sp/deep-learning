= Terminologies and Notations =
:doctype: book
:author: Bharath Kumar S P
:email: bharath030195@gmail.com
:stem: latexmath
:eqnums:
:toc:

== Why a Network Model? ==
In machine learning, we mathematically generate linear models (linear with respect to the parameters stem:[w_1, \dots, w_M]). These models are linear combinations of fixed basis functions stem:[\phi_1(\mathbf{x}), \dots, \phi_M(\mathbf{x})].

[stem]
++++
\hat{y} = f(w_1 \phi_1(\mathbf{x}) + \dots + w_M \phi_M(\mathbf{x}) + b) = f(\boldsymbol{\phi}^\top(\mathbf{x}) \mathbf{w} + b)
++++

For regression, stem:[f] is the identity function, stem:[f(x)=x]. We fix these basis functions as per our need. It can be linear functions to get linear curve or non-linear functions to get non-linear curve.

[stem]
++++
\begin{align*}
\hat{y} & = w_1 X_1 + w_2 X_2 + b && \text{(linear)} \\
\hat{y} & = w_1 X_1 + w_2 X_1 X_2 + w_3 X_1^2 + \dots + b && \text{(non-linear)}
\end{align*}
++++

For binary classification, stem:[f] is the sigmoid function. We fix these basis functions as per our need. It can be linear functions to get linear decision boundary or non-linear functions to get non-linear decision boundary.

We fit the model to the training data to estimate the parameters stem:[\mathbf{w}, b]. We then use this model to predict the target value for the test data. These fixed basis functions don't scale easily to complex settings. But it would be really helpful if we could adapt the basis functions to the data. We can:

* Fix the number of basis functions, stem:[M].
* But allow them to be adaptive, i.e., we don't assign a specific functional form to each basis function. We express the basis functions in parameteric form and we learn them based on the task and data.

To achieve this, we can use a network model instead of a mathematical model for regression and classification. We will make each basis function depend on a set of learnable parameters. Each basis function stem:[\phi_j(\mathbf{x})] itself is a linear combination of inputs passed through a non-linear function.

Here we made recursion. Till now, we were modelling our output as the linear combination of the inputs passed through a linear (in the case of regression) or non-linear function (sigmoid in the case of regression). Now we are modelling each stem:[\phi_j]'s themselves as linear combination of the inputs passed through a non-linear function. Now each stem:[\phi_j] will have its own set of parameters, and stem:[w_j]'s are another set of parameters that are used to combine all the basis functions.

We fit this network model to the training data to determine stem:[\mathbf{w}, b], and predict the target value of the test data.

image::.\images\network_model_01.png[align='center', 600, 400]

Using a network model instead of a mathematical model has many advantages:

. Even small changes to the mathematical model make it difficult to solve, but the network model can be modified to any extent.
. To create a complex model, we can just increase the hidden layers and the number of nodes in them in the network model.

== Basic Structure of ANN ==
Neural Networks are supervised learning techniques. ANNs are applications of feed forward networks and neural systems. The basic structure of ANN is

* The number of nodes in the input layer is determined by the number of features in the data.
* The number of nodes in the output layer is determined by the number of target values in the data.
* The number of nodes in the hidden layer is a hyperparameter. This ANN has a total of 9 nodes (each node can be given a number).
* This network is said to have two layers (input to hidden, hidden to output) or one hidden layer. The below network is called as a two-layer network, because it is the number of layers of adaptive weights that is important for determining the network properties. Whenever there is a transformation, we can call it as a layer. The neural network is also known as the multilayer perceptron, or MLP.
* Depth of a ANN: How deep is the output from the input? The maximum number of steps from input to reach output defines the depth of the network, the depth of this network is 2. 

image::.\images\ann_structure_01.png[align='center', 700, 500]

The components of ANN are

=== Artificial neuron ===

The nodes in the input layer pass the input signal as is to the upper node. The nodes in the hidden layer and output layer add all input signals received from lower nodes, convert the result using function stem:[f], and output it. A bias node is connected to each node in the hidden layer and output layer. This always outputs 1.

=== Connection weights ===
Connect two nodes stem:[(i,j)] with a weight stem:[w] or stem:[b]. The output of the lower neuron is multiplied by the weight and passed to the upper neuron. The weights stem:[w] and stem:[b] can be considered as the synaptic gap, i.e. the small gap between the output of the lower neuron and the input of the higher neuron.

** When stem:[w] or stem:[b] is small, i.e. when the gap is wide, only a small portion of the information from the lower neuron is passed on to the upper neuron.
** When stem:[w] is large, i.e. when the gap is narrow, much of the information is passed on to the upper neuron.

Input signal (or total amount of information received) of neuron 6 stem:[= x_1 * w_{06} + x_2 * w_{16} + 1 * b_{23}]

stem:[w_{ij}] and stem:[b_{ij}] denotes the connection weight between node stem:[i] and node stem:[j]. If stem:[w_{06}=0.1], only 10% of the input signal stem:[x_1] is passed to the upper node. 

=== Activation function ===
Each neuron has a activation function which controls the output size of the neuron. Neurons in the hidden layer use non-linear activation function stem:[f_h], and neurons in the output layer use linear or non-linear activation function stem:[f_o].

Output signal of neuron 6 stem:[= f_h(x_1 * w_{06} + x_2 * w_{16} + 1 * b_{23})]

Activation function is used to convert the output of a neuron into a desired range of values. For example, in the case of binary classification, a sigmoid function or a hyperbolic tangent is used in the output layer because the output value must be between 0 and 1, or between -1 and +1. And in the case of regression, a linear function is used because the output value should not change.

Another purpose of the activation function is to introduce non-linearity into the hidden layer. If we do not use an activation function or use a linear function in the hidden layer, the network behaves as a single-layer neural network. For non-linear learning, a non-linear function must be used in the hidden layer.

ReLU (Rectified Linear Unit), tanh, leaky ReLU, softplus, etc. are used in the hidden layer for non-linear learning. ReLU is most widely used in the hidden layer.

image::.\images\activation_functions.png[align='center', 700, 500]

In the output neuron, the type of activation used depends on our problem:

* For standard regression problems, the activation function is the identity, stem:[y_k = a_k].
* For multiple binary classification problems, each output unit activation is transformed using a logistic sigmoid function, stem:[y_k = \sigma(a_k)].
* For multiclass problems, a softmax activation function is used.

== Notations ==
Consider a MLP network as below:

.Image Source: http://neuralnetworksanddeeplearning.com/chap2.html[Chapter 2 in Michael Nielson's NNDL textbook]
image::.\images\sample_mlp.png[align='center', 300, 300]

* stem:[w^l_{jk}] is the weight connecting to the stem:[l]th layer, that connects the stem:[j]th neuron in the stem:[l]th layer and stem:[k]th neuron in the stem:[l-1]th layer.
* stem:[b^l_j] is the bias of the stem:[j]th neuron in the stem:[l]th layer.
* stem:[x^l_j] is the output of the stem:[j]th neuron in stem:[l]th layer. stem:[\sigma(.)] is the activation function.
+
[stem]
++++
x^l_j = \sigma\left( \sum_k w^l_{jk} x^{l-1}_k + b^l_j \right)
++++
+
where the sum is over all neurons stem:[k] in the stem:[(l-1)]th layer.

* Vector of activations (or, biases) at layer stem:[l] is denoted by a bold-faced stem:[\mathbf{x}^l] (or stem:[\mathbf{b}^l]) and stem:[\mathbf{W}^l] is the weight matrix connecting to the stem:[l]th layer. Weights connecting to the first neuron of the stem:[l]th layer forms the first row in the matrix, weights connecting to the second neuron forms the second row, etc. The entry in the stem:[j]th row and stem:[k]th column is stem:[w^l_{jk}].
+
Say we have stem:[J] neurons in the stem:[l]th layer and stem:[K] neurons in the stem:[(l-1)]th layer. Then matrix stem:[\mathbf{W}^l] will be of size stem:[J \times K]. stem:[\mathbf{x}^l] will be stem:[K \times 1] and stem:[\mathbf{b}^l] will be stem:[J \times 1].
+
NOTE: All these stem:[\mathbf{x}^l, \mathbf{b}^l, \mathbf{W}^l ] are defined for each layer stem:[l]. And stem:[\sigma(.)] is applied element-wise.
+
[stem]
++++
\mathbf{x}^l = \sigma\left( \mathbf{W}^l \mathbf{x}^{l-1} + \mathbf{b}^l \right)
++++

* stem:[s^l_j] is the weighted input to the stem:[j]th neuron in the stem:[l]th layer. This quantity refers to pre-activation.
+
[stem]
++++
\begin{align*}
s^l_j & =  \sum_k w^l_{jk} x^{l-1}_k + b^l_j && \text{for the } j \text{th neuron in the } l \text{th layer} \\
\mathbf{s}^l & =  \mathbf{W}^l \mathbf{x}^{l-1} + \mathbf{b}^l && \text{for the } l \text{th layer}  \\
\end{align*}
++++


