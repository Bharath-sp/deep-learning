= Neural Networks =
:doctype: book
:author: Bharath Kumar S P
:email: bharath030195@gmail.com
:stem: latexmath
:eqnums:
:toc:

== Basic Neural Network ==
Basic neural network model as a series of transformations.

We are given a set of input features stem:[X_1, \dots, X_D]. Now let's consider a stem:[M] linear combinations of these input features. One of the linear combinations, say stem:[j]th will be:

[stem]
++++
a_j = w_{j0} X_0 + w_{j1} X_1 + \dots + w_{jD} X_D
++++

In terms of each data point, where stem:[\mathbf{x} = (x_1, x_2, \dots, x_D)].

[stem]
++++
a_j = w_{j0} + w_{j1} x_1 + \dots + w_{jD} x_D = w_{j0} + \sum_{i=1}^D w_{ji} x_i = \mathbf{w}_j^\top \mathbf{x}
++++

NOTE: stem:[w_{j0}] is called as the bias term, it doesn't depend on the input variables.

This is one of the stem:[M] combinations, so stem:[j=1, \dots, M]. To indicate that the parameters are in the first `layer` of the network, we use the superscript (1):

[stem]
++++
a_j = w_{j0}^{(1)} + \sum_{i=1}^D w_{ji}^{(1)} x_i
++++

We refer to the parameters stem:[w_{ji}^{(1)}] as weights and the parameters stem:[w_{j0}^{(1)}] as biases. The quantitites stem:[a_j]'s are known as `activations`. Each of them is then transformed using a differentiable, nonlinear activation function stem:[h(.)] such as sigmoid, tanh, ReLU functions, etc. to give

[stem]
++++
z_j = h(a_j)
++++

In matrix notation, this can be written as:

[stem]
++++
\begin{bmatrix}
z_1 \\
z_2 \\
\vdots \\
z_M
\end{bmatrix}_{M \times 1} = h \left(
\begin{bmatrix}
w_{10}^{(1)} & w_{11}^{(1)} & \dots & w_{1D}^{(1)} \\
w_{20}^{(1)} & w_{21}^{(1)} & \dots & w_{2D}^{(1)} \\
\vdots & \vdots & \vdots & \vdots \\
w_{M0}^{(1)} & w_{M1}^{(1)} & \dots & w_{MD}^{(1)} \\
\end{bmatrix}_{M \times (D+1)}
\begin{bmatrix}
x_0 \\
x_1 \\
\vdots \\
x_D
\end{bmatrix}_{(D+1 )\times 1} \right)
++++

The first column of the matrix is the bias parameters and other columns are the weight parameters.

stem:[z_j]'s are called as hidden units or hidden neurons. We get stem:[M] basis functions as we had stem:[M] linear combinations. Note: activation is a linear functions which is passed through a non-linear function to form a basis function.


The values stem:[z_j]'s are again linearly combined to give output unit activations. This transformation corresponds to the second layer of the network.

[stem]
++++
a_k = w_{k0}^{(2)} + \sum_{j=1}^M w_{kj}^{(2)} z_j
++++

Where stem:[k=1,\dots, K], and stem:[K] is the total number of outputs. Finally, the output unit activations are transformed through an appropriate activation function to give a set of outputs stem:[y_k]. The basis function stem:[z_j]'s are linearly combined according to some weights and passed through stem:[f] to get the output stem:[y_k].

[stem]
++++
y_k = f(a_k)
++++

For stem:[k=1, \dots, K], this can be written in matrix notation as:

[stem]
++++
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_K
\end{bmatrix}_{K \times 1} = f \left(
\begin{bmatrix}
w_{10}^{(2)} & w_{11}^{(2)} & \dots & w_{1M}^{(2)} \\
w_{20}^{(2)} & w_{21}^{(2)} & \dots & w_{2M}^{(2)} \\
\vdots & \vdots & \vdots & \vdots \\
w_{K0}^{(2)} & w_{M1}^{(2)} & \dots & w_{KM}^{(2)} \\
\end{bmatrix}_{K \times (M+1)}
\begin{bmatrix}
z_0 \\
z_1 \\
\vdots \\
z_M
\end{bmatrix}_{(M+1) \times 1} \right)
++++

image::.\images\neural_network_01.png[align='center']

* For standard regression problems, the activation function is the identity, stem:[y_k = a_k].
* For multiple binary classification problems, each output unit activation is transformed using a logistic sigmoid function, stem:[y_k = \sigma(a_k)].
* For multiclass problems, a softmax activation function is used.

*Layer terminology:* The above network is called as a two-layer network, because it is the number of layers of adaptive weights that is important for determining the network properties. Whenever there is a transformation, we can call it as a layer. The neural network is also known as the multilayer perceptron, or MLP.

We can combine these two stages to give the overall network function

[stem]
++++
y_k(\mathbf{x}, \mathbf{w}) = f \left( w^{(2)}_{k0} + \sum_{j=1}^M w^{(2)}_{kj} \, h \left( w^{(1)}_{j0} + \sum_{i=1}^D w^{(1)}_{ji} x_i  \right) \right)
++++

where the set of all weight and bias parameters have been grouped together into a vector stem:[\mathbf{w}].

This function is represented in the form of a network diagram above. The bias parameter can be absorbed into the set of weight parameters by defining an additional input variable stem:[x_0] whose value is clamped at stem:[x_0=1].

[stem]
++++
y_k(\mathbf{x}, \mathbf{w}) = f \left( w^{(2)}_{k0} + \sum_{j=1}^M w^{(2)}_{kj} \, h \left( \sum_{i=0}^D w^{(1)}_{ji} x_i \right) \right)
++++


Similarly, we can absorb the second-layer biases into the second-layer weights by clamping, stem:[z_0 = h(a_0) = 1], where

[stem]
++++
z_0 = h(a_0) = h \left( w^{(1)}_{00} \, x_0 + w^{(1)}_{01} \, x_1 + \dots + w^{(1)}_{0D} \, x_D \right) = 1
++++

So that the overall network function becomes

[stem]
++++
y_k(\mathbf{x}, \mathbf{w}) = f \left( \sum_{j=0}^M w^{(2)}_{kj} \, h \left( \sum_{i=0}^D w^{(1)}_{ji} x_i \right)  \right)
++++

For stem:[k=1, \dots, K], this can be written in matrix notation as:

[stem]
++++
\begin{align*}
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_K
\end{bmatrix}_{K \times 1} 
& = f \left(
\begin{bmatrix}
w_{10}^{(2)} & w_{11}^{(2)} & \dots & w_{1M}^{(2)} \\
w_{20}^{(2)} & w_{21}^{(2)} & \dots & w_{2M}^{(2)} \\
\vdots & \vdots & \vdots & \vdots \\
w_{K0}^{(2)} & w_{K1}^{(2)} & \dots & w_{KM}^{(2)} \\
\end{bmatrix}_{K \times (M+1)}

\begin{bmatrix}
h \left( w^{(1)}_{00} \, x_0 + w^{(1)}_{01} \, x_1 + \dots + w^{(1)}_{0D} \, x_D \right) \\
h \left( w^{(1)}_{10} \, x_0 + w^{(1)}_{11} \, x_1 + \dots + w^{(1)}_{1D} \, x_D \right) \\
\dots \\
h \left( w^{(1)}_{M0} \, x_0 + w^{(1)}_{M1} \, x_1 + \dots + w^{(1)}_{MD} \, x_D \right)
\end{bmatrix}_{(M+1 )\times 1} \right) \\
\\

& = f \left(
\begin{bmatrix}
w_{10}^{(2)} & w_{11}^{(2)} & \dots & w_{1M}^{(2)} \\
w_{20}^{(2)} & w_{21}^{(2)} & \dots & w_{2M}^{(2)} \\
\vdots & \vdots & \vdots & \vdots \\
w_{K0}^{(2)} & w_{M1}^{(2)} & \dots & w_{KM}^{(2)} \\
\end{bmatrix}_{K \times (M+1)}
h \left(
\begin{bmatrix}
w_{00}^{(1)} & w_{01}^{(1)} & \dots & w_{0D}^{(1)} \\
w_{10}^{(1)} & w_{11}^{(1)} & \dots & w_{1D}^{(1)} \\
\vdots & \vdots & \vdots & \vdots \\
w_{M0}^{(1)} & w_{M1}^{(1)} & \dots & w_{MD}^{(1)} \\
\end{bmatrix}_{(M+1) \times (D+1)}
\begin{bmatrix}
x_0 \\
x_1 \\
\vdots \\
x_D
\end{bmatrix}_{(D+1 )\times 1} \right) \right) \\
\end{align*}
++++

This can be written as

[stem]
++++
\mathbf{y} = f(\mathbf{W}_1 \, h(\mathbf{W}_2 \mathbf{x}))
++++

Thus the neural network model is simply a nonlinear function from input stem:[\mathbf{x}] to output stem:[\mathbf{y}] controlled by a vector stem:[\mathbf{w}] of adjustable parameters, where stem:[\mathbf{w}] is the long vector of parameters.

=== Number of Parameters ===

The parameters in first row of matrix stem:[\mathbf{W}_2] are dummy. The first column in both the matrix are bias parameters. So in total there are stem:[(M * D) + (K * M)] weight parameters and stem:[M+K] bias parameters.

For instance, say we have neural network with 3 layers. And stem:[\mathbf{x} \in \mathbb{R}^{784}].

image::.\images\neural_network_02.png[align='center', 500, 400]

* First layer nodes: 16
* Second layer nodes: 16
* Output nodes: 10

There are stem:[(784 * 16) + (16*16) + (16 * 10)] weight parameters. And stem:[16 + 16 + 10] bias parameters. In total, there are 13,002 parameters.

During the network training, we need to learn all these parameters. We should ensure that the parameters across stem:[M] linear combinations are different, i.e., the basis functions learned are different. Number of layers, number of linear combinations stem:[M] in each layer are hyperparameters which should be tuned through cross-validation.