= ANN Training =
:doctype: book
:author: Bharath Kumar S P
:email: bharath030195@gmail.com
:stem: latexmath
:eqnums:
:toc:

== Preparation ==

* *Data normalization or standardization:*
+
Data normalization is essential for training artificial neural network models. When the features are in different scales, the output value is more affected by the features with large scale. Methods for normalizing data include min-max normalization or z-score normalization.

* *Data shuffling:*
+
Data shuffling is not required when using batch gradient descent, but is required when using stochastic or mini-batch method. Let's assume we have a dataset collected by country. We want to train on this dataset using mini-batch gradient descent. Subsets 1 and 2 both contain the data points from country A. These data points are highly correlated. If the neural network trains on this data sequentially, it will first train on the data from country A, then the data from countries B, C, and D. When training on data from country D, the network may forget the knowledge it previously trained on country A.
+
To prevent this, data shuffling is required. Because each subset contains data points from different countries, the correlation becomes smaller.
+
image::.\images\data_shuffling.png[align='center', 400, 300]

* *Use GPUs:*
+
Neural networks primarily perform large-scale matrix multiplication repetitively. The more features in the training data and the more neurons in the hidden layer, the larger the matrix stem:[\mathbf{W}] becomes, and the computation cost of multiplication increases exponentially. GPUs have numerous cores and can perform matrix multiplication in parallel, allowing matrix multiplication to be performed very quickly. Thus, GPUs are essential for training on large amounts of data for deep learning.
+
Deep learning tools like TensorFlow or PyTorch natively use GPUs to perform the matrix multiplication.



