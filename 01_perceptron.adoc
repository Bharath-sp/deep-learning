= Perceptron =
:doctype: book
:author: Bharath Kumar S P
:email: bharath030195@gmail.com
:stem: latexmath
:eqnums:
:toc:

== What is a Perceptron? ==
In 1957, Frank Rosenblatt came up with a more generic model. This model also performs linear classification, but the inputs can be real stem:[-\infty] to stem:[\infty] and different weights can be given to different input components. This model gives us a mechanism to provide more importance to some inputs and less to others. And also a mechanism to learn the weights.

The output of the model is 1 when the weighted combination of the inputs is greater than a threshold. The outputs are binary.

[stem]
++++
f(\mathbf{x}) = \begin{cases} 1 & \text{when } \sum_i w_ix_i +b \geq 0 \text{ or } \sum_i w_ix_i \geq -b \\ 0 & \text{else} \end{cases}
++++

Here stem:[\mathbf{w}, b] are all parameters to learn. Consider a step function stem:[\sigma(x)], whose behaviour is as

image::.\images\perceptron_01.png[align='center',  600, 400]

Using this step function, our function stem:[f(\mathbf{x})] can be compactly written as

[stem]
++++
f(\mathbf{x}) = \sigma(\mathbf{w}^\top \mathbf{x} + b)
++++

In general, stem:[\sigma(.)] is called as activation function. Here we have used a step function. A single Perceptron can be described as

image::.\images\perceptron_02.png[align='center',  500, 400]

Here stem:[w_0 = b] and stem:[x_0=1]. The bias term denotes the overall bias in the output when all the inputs are zero.

== Perceptron Learning Algorithm ==
It is a supervised learning algorithm. We are given a training data stem:[\{(\mathbf{x}_i, t_i)\}_{i=1}^N], where stem:[\mathbf{x}_i \in \mathbb{R}^D] and stem:[t_i \in \{-1,1\}]. Let stem:[k] capture the number of updates.

. Start with stem:[k \leftarrow 0] and stem:[\mathbf{w}_k = \mathbf{0}]. Here stem:[\mathbf{w}_k \in \mathbb{R}^{D+1} ], which includes the bias term as well. Also consider stem:[\mathbf{x}_i \in \mathbb{R}^{D+1}] by adding a constant 1.
. While stem:[\exists] stem:[i \in \{1,2,\dots, N\}] such that stem:[t_i(\mathbf{w}_k^\top \mathbf{x}_i) \leq 0], update
+
[stem]
++++
\begin{align*}
\mathbf{w}_{k+1} & = \mathbf{w}_k + t_i \mathbf{x}_i \\
k & \leftarrow k+1
\end{align*}
++++
+
While there exists a misclassification, we perform the above updates. The condition stem:[t_i(\mathbf{w}_k^\top \mathbf{x}_i) \leq 0] checks for any misclassification. It is to ensure that both the ground truth and the prediction have the same sign. 

When current stem:[\mathbf{w}_k] is misclassifying any of the data points, we perform an update: stem:[\mathbf{w}_{k+1} = \mathbf{w}_k + t_i \mathbf{x}_i]. What does this update do?

*Geometric View:*

We know stem:[\mathbf{w}_k^\top \mathbf{x}_i = \mathbf{\tilde{w}}_k^\top \mathbf{x}_i + w_0], where 

* stem:[\mathbf{w} = (w_0, w_1, \dots, w_D)] and stem:[\mathbf{x} \in \mathbf{R}^{D+1}] in the LHS.
* stem:[\mathbf{\tilde{w}} = (w_1, \dots, w_D)] and stem:[\mathbf{x} \in \mathbf{R}^D] in the RHS.

The decision surface stem:[\mathbf{w}_k^\top \mathbf{x}_i], shown in blue, is always perpendicular to stem:[\mathbf{\tilde{w}}_k] and its displacement from the origin is controlled by the bias parameter stem:[w_0].

image::.\images\perceptron_03.png[align='center',  400, 300]

Say we have a point stem:[\mathbf{x}_i] with the true label as stem:[t_i=-1] in the right side of our decision boundary. As per the weights learned, stem:[\mathbf{w}_k^\top \mathbf{x}] is stem:[>0] for this point, which makes stem:[t_i(\mathbf{w}_k^\top \mathbf{x}_i) < 0]. Therefore, it is a misclassification.

The update step can be seen geometrically as

. stem:[\mathbf{x}_i] is multiplied with stem:[t_i], in this case as stem:[t_i=-1], this becomes stem:[-\mathbf{x}_i]. Note here stem:[\mathbf{x}_i \in \mathbb{R}^2].
. The vector stem:[-\mathbf{x}_i] is added to stem:[\mathbf{\tilde{w}}_k] to get stem:[\mathbf{\tilde{w}}_{k+1}].
. Th intercept term stem:[w_{0,k}] is updated to stem:[w_{0,k+1} = w_{0,k} + (t_i* 1) = w_{0,k} -1].
. The decision boundary is set orthogonal to stem:[\mathbf{\tilde{w}}_{k+1}].

image::.\images\perceptron_04.png[align='center',  800, 500]

Say we have a point stem:[\mathbf{x}_j] with the true label as stem:[t_i=1] in the left side of our decision boundary. As per the weights learned, stem:[\mathbf{w}_k^\top \mathbf{x}] is stem:[<0] for this point, which makes stem:[t_i(\mathbf{w}_k^\top \mathbf{x}_i) < 0]. Therefore, it is also a misclassification. The update step is carried out to correct this misclassification.

On each update, the decision boundary is re-aligned to correct the misclassification. This process is repeated for every misclassified data point.

*Mathematical View:*

The condition to check for misclassification is stem:[t_i(\mathbf{w}_k^\top \mathbf{x}_i) \leq 0].

Case 1: When stem:[t_i = 1], stem:[\mathbf{w}_k^\top \mathbf{x}_i \leq 0]. The update equation is

[stem]
++++
\begin{align*}
\mathbf{w}_{k+1} & = \mathbf{w}_k + \mathbf{x}_i \\
\mathbf{w}_{k+1}^\top \mathbf{x}_i & = \mathbf{w}_k^\top  \mathbf{x}_i + \mathbf{x}_i^\top \mathbf{x}_i && \text{On taking dot product with } \mathbf{x}_i \\
\mathbf{w}_{k+1}^\top \mathbf{x}_i & = \mathbf{w}_k^\top  \mathbf{x}_i + \|\mathbf{x}_i\|^2
\end{align*}
++++

We are adding a positive quantity stem:[\|\mathbf{x}_i\|^2] to stem:[\mathbf{w}_k^\top \mathbf{x}_i]. If the positive quantity is large enough, this makes the term stem:[\mathbf{w}_k^\top \mathbf{x}_i] become stem:[>0]. Thus adjusting the decision boundary to correct the misclassification.

Case 2: When stem:[t_i = -1], stem:[\mathbf{w}_k^\top \mathbf{x}_i \geq 0]. The update equation is

[stem]
++++
\begin{align*}
\mathbf{w}_{k+1} & = \mathbf{w}_k - \mathbf{x}_i \\
\mathbf{w}_{k+1}^\top \mathbf{x}_i & = \mathbf{w}_k^\top  \mathbf{x}_i - \mathbf{x}_i^\top \mathbf{x}_i && \text{On taking dot product with } \mathbf{x}_i \\
\mathbf{w}_{k+1}^\top \mathbf{x}_i & = \mathbf{w}_k^\top  \mathbf{x}_i - \|\mathbf{x}_i\|^2
\end{align*}
++++

We are subtracting a positive quantity stem:[\|\mathbf{x}_i\|^2] from stem:[\mathbf{w}_k^\top \mathbf{x}_i]. If the positive quantity is large enough, this makes the term stem:[\mathbf{w}_k^\top \mathbf{x}_i] become stem:[<0]. Thus adjusting the decision boundary to correct the misclassification.

=== Convergence ===

====
For a linearly separable dataset, the algorithm will converge within a finite number of updates, i.e., when there is a line which is possible to give 100% classification accuracy, this algorithm will find it in a finite number of updates.
====

NOTE: Here one update refers to choosing a misclassified data point and updating the boundary.

On a practical note, we say that a algorithm is converged when either of the following happens:

* We get 100% training accuracy. And when there are multiple linear boundaries possible with 100% accuracy, the perceptron learning algorithm treats all of them equal, and returns any one of them.
* We reach maximum number of updates specified.
* When the difference stem:[\| \mathbf{w}_{k+1} - \mathbf{w}_k \|] is very small stem:[< \epsilon] for all misclassified stem:[\mathbf{x}_i]'s. 

The factors that influence the number of updates stem:[k], i.e., the convergence of this algorithm are

* The margin between the positive and the negtive class data points stem:[\gamma].
* How far the samples are from the origin, i.e., the radius stem:[R] of the circle which is encompassing the largest data point.

For linearly separable data, it can be shown that stem:[k < \frac{R^2}{\gamma^2}]. The Perceptron Learning Algorithm makes at most stem:[\frac{R^2}{\gamma^2}] updates (after which it returns a separating hyperplane).

== References ==

. McCulloch-Pitts neurons. (n.d.). https://mind.ilstu.edu/curriculum/mcp_neurons/index.html
. Nielsen, M. A. (2015). Neural networks and deep learning. http://neuralnetworksanddeeplearning.com/chap1.html
. Shivaram Kalyanakrishnan. (2017). The Perceptron Learning Algorithm and its Convergence. https://www.cse.iitb.ac.in/~shivaram/teaching/old/cs344+386-s2017/resources/classnote-1.pdf 