= Operations of ANN =
:doctype: book
:author: Bharath Kumar S P
:email: bharath030195@gmail.com
:stem: latexmath
:eqnums:
:toc:

== Prediction Process of ANN ==
There are two steps in the operation of ANN: Forward propagation and backpropagation. The ANN training process requires both steps, but the prediction process requires only the forward propagation step. Say we have a network as below which is trained for a binary classification problem with data stem:[x_1, x_2, y].

image::.\images\ann_network_01.png[align='center', 600, 400]

Here we have 

[stem]
++++
\mathbf{X} = \begin{bmatrix}
x_{11} & x_{12} \\
x_{21} & x_{22} \\
\vdots & \vdots \\
x_{N1} & x_{N2} \\
\end{bmatrix}_{N \times 2} \hspace{1cm} \mathbf{W}_h = \begin{bmatrix}
w_{13} & w_{14} & w_{15} & w_{16} \\
w_{23} & w_{24} & w_{25} & w_{26}
\end{bmatrix}_{2 \times 4} \hspace{1cm} \mathbf{b}_h = \begin{bmatrix}
b_{03} \\
b_{04} \\
b_{05} \\
b_{06}
\end{bmatrix}^\top_{1 \times 4}
++++

Broadcasting occurs when we add stem:[\mathbf{X} \cdot \mathbf{W}_h + \mathbf{b}_h] and the result is a stem:[N \times 4] matrix. This matrix is passed to the activation functions stem:[f_h]. The result is

[stem]
++++
f_h(\mathbf{X} \cdot \mathbf{W}_h + \mathbf{b}_h) = \mathbf{H} = \begin{bmatrix}
h_{11} & h_{12} & h_{13} & h_{14} \\
h_{21} & h_{22} & h_{23} & h_{24} \\
\vdots & \vdots & \vdots & \vdots \\
h_{N1} & h_{N2} & h_{N3} & h_{N4}
\end{bmatrix}_{N \times 4}
++++

Each row in stem:[\mathbf{H}] is an output from the hidden layers for a single data point.

[stem]
++++
\mathbf{W}_o = \begin{bmatrix}
w_{38} \\
w_{48} \\
w_{58} \\
w_{68}
\end{bmatrix}_{4 \times 1} \hspace{1cm} \mathbf{b}_o = b_{78}
++++

We do stem:[\mathbf{H} \cdot \mathbf{W}_o  + \mathbf{b}_o], which is stem:[N \times 1] vector. This is passed to the sigmoid function stem:[f_o], the result is a which is stem:[N \times 1] vector. Thus the final output of the neural network is

[stem]
++++
\hat{y} = f_o(\mathbf{H} \cdot \mathbf{W}_o  + \mathbf{b}_o) = f_o(f_h(\mathbf{X} \cdot \mathbf{W}_h + \mathbf{b}_h) \cdot \mathbf{W}_o + \mathbf{b}_o)
++++

Once ANN training is complete, stem:[\mathbf{W}_h, \mathbf{b}_h, \mathbf{W}_o, \mathbf{b}_o] are determined and the network is ready to make predictions. And say we use ReLU activation functions for the neurons in the hidden layer and sigmoid function for the neuron in the output layer.

image::.\images\ann_pred_implement.png[align='center', 900, 600]

The accuracy of this neural network is 100%.

== Objective Function ==
The parameters of a neural network must be determined through a training process. In order to train an ANN, a loss function, which is the objective function, is needed.

* In regression analysis, the MSE is used as the loss function.
+
[stem]
++++
L(\mathbf{w},b) = \frac{1}{N} \sum_{i=1}^N \frac{1}{2} (y_i - \hat{y}_i)^2
++++

* In binary classification, binary cross-entropy (BCE) is used.
+
[stem]
++++
L(\mathbf{w},b) = - \frac{1}{N} \sum_{i=1}^N [y_i \log(\hat{y}_i) + (1-y_i) \log (1-\hat{y}_i)]
++++

* In multiclass classification, cross-entropy (CE) is used.
+
[stem]
++++
L(\mathbf{w},b) = - \frac{1}{N} \sum_{i=1}^N \sum_{k=1}^C y_{i,k} \log(\hat{y}_{i,k})
++++
+
Where stem:[C] is the number of classes in stem:[y], and it is the number of neurons in the output layer.

NOTE: stem:[y_i] and stem:[\hat{y}_i] are enough to calculate these loses.

Regression uses a linear activation function for the neurons in the output layer, and binary classification uses a sigmoid activation function. And multiclass classification uses a softmax activation function.

Our goal is to find the parameters stem:[\mathbf{w}] and stem:[b] that make the loss function as small as possible.

[stem]
++++
\min_{\mathbf{w},b} L(\mathbf{w},b)
++++

NOTE: When the accuracy for two models A and B is the same, the model with a low BCE/CE or MSE is said to have better performance than the other model. For a data point with the actual target value 0, model A can have predicted probability 0.1 to get class 0, while model B can have 0.49 as the predicted probability to get class 0. In this case, model A will have low BCE.

== Why Cross-Entropy is Used ==
Let's understand cross entropy from the perspective of information theory. In information theory, the amount of information (or "surprise") associated with observing an event stem:[X=x] is inversely proportional to its probability stem:[p_X(x)].

* The amount of information with high probability events is small. They are less surprising and carry less information.
* The amount of information with low probability events is large. They are more surprising and carry more information.

The information content (surprisal) of an event stem:[X=x] is a function that increases as the probability stem:[p_X(x)] of the event decreases. When stem:[p(x)] is close to 1, the surprisal of the event is low, but if stem:[p(x)] is close to 0, the surprisal of the event is high. This relationship is described by the function stem:[\log \frac{1}{p(x)}]. Hence, we can define the information of an event stem:[X=x] as

[stem]
++++
I(X=x) = \log_2 \left( \frac{1}{p(x)} \right)
++++

The information content of a random variable as stem:[I(X) =  \log_2 \left( \frac{1}{p(X)} \right) = - \log_2(p(X))].

CAUTION: The information content is measured for the probability distribution stem:[p_X(x)] of the random variable stem:[X], not directly for stem:[X] itself.

=== Entropy ===
Entropy of the probability distribution stem:[p] of a random variable is the average information content over all possible outcomes, weighted by their probabilities.

Let stem:[X] be a discrete random variable which takes value in the set stem:[\mathcal{X}] and distributed according to stem:[p: \mathcal{X} \rightarrow [0,1\]] such that stem:[p(x):= P({X=x})]. The entropy of the probability distribution stem:[p] of random variable stem:[X] is a measure of average uncertainty or unpredictability associated with the random variable. It is defined as

[stem]
++++
H(p) = H(X) = \mathbb{E}_X [ I(X)] = - \mathbb{E}_X [\log_2(p(X))] = - \sum_{x \in \mathcal{X}} p(x) \cdot \log_2(p(x))
++++

It quantifies the amount of information required, on average, to describe or encode the outcomes of the random variable.

* If the random variable stem:[X] has high entropy, its outcomes are highly uncertain or spread out.
* If stem:[X] has low entropy, its outcomes are predictable or concentrated around a single value.

*Example:*

. Let stem:[\mathcal{X} = \{0,1\}] and the probability distribution stem:[p=[0.5, 0.5\]]. Then entropy stem:[H(X) = -(0.5 \log 0.5 + 0.5 \log 0.5) = 1] bit. On average, we need 1 bit to describe the outcome of stem:[X].

. Let stem:[\mathcal{X} = \{0,1\}] and the probability distribution stem:[q=[0.1, 0.9\]]. Then entropy stem:[H(X) = -(0.9 \log 0.9 + 0.1 \log 0.1) \approx 0.47] bits. On average, we need only 0.47 bits to describe the outcome of stem:[X].

=== KL Divergence ===
Say that the true class distribution is stem:[p_X(x)], the average amount of information needed to describe (stem:[x] under) stem:[p] is the entropy stem:[H(p)],

[stem]
++++
H(p) = - \sum_x p(x) \cdot \log_2(p(x))
++++

This represents the true uncertainty in the probability distribution of stem:[X].

If instead of the true distribution stem:[p_X(x)], we use the predicted distribution stem:[q_X(x)], then the amount of information needed to describe stem:[X] is stem:[- \log q_X(x)]. The average amount of information to describe the outcomes of the random variable stem:[X] or the probability distribution stem:[q] is

[stem]
++++
H(q) =  - \mathbb{E}_X [\log_2(q(X))] = - \sum_x p(x) \cdot \log_2(q(x))
++++

The KL divergence measures the expected additional information required when using the predicted distribution stem:[q_X(x)] instead of the true distribution stem:[p_X(x)]. It is defined as

[stem]
++++
D_{KL}(p \parallel q) = \mathbb{E}_X \left[ \log \frac{p_X(x)}{q_X(x)} \right] = - \sum_x p(x) \log q(x) + \sum_x p(x) \log p(x)
++++

For each possible outcome stem:[x], the difference stem:[\log p_X(x) - \log q_X(x) ] measures how much extra surprise is introduced by stem:[q] compared to stem:[p]. If stem:[q] matches stem:[p] perfectly, the KL divergence is 0 because no additional information is needed. The larger the divergence, the greater the inefficiency in using stem:[q] to describe the true distribution stem:[p]. The value of KL divergence will always be stem:[\geq 0].

In machine learning, KL Divergence is the expected difference between the amount of information required in describing the random variable stem:[C] (which denotes classes) under the true distribution stem:[y] and the predicted distribution stem:[\hat{y}]. Here

* stem:[y] is the actual class probability distribution. For example, say our target variable has three classes, stem:[C] takes three values, stem:[i=0,1,2]. Then for a data point, the actual distribution can be stem:[y= [1,0,0\]].
* stem:[\hat{y}] is the predicted class probability distribution. For the data point, the predicted distribution can be stem:[\hat{y}= [0.7, 0.2, 0.1\]].

The difference in the amount of information between stem:[y] and stem:[\hat{y}] is

[stem]
++++
\Delta I = \log \frac{1}{\hat{y}} - \log \frac{1}{y} = - \log(\hat{y}) + \log(y)
++++

The smaller the difference, the more similar the two distributions are.

[stem]
++++
D_{KL}(y \parallel \hat{y}) = \mathbb{E}_C[\Delta I] = - \sum_i y_i \log (\hat{y}_i) + \sum_i y_i \log (y_i)
++++

=== Cross-Entropy ===
Minimizing KL divergence aligns predicted distribution stem:[\hat{y}] as closely as possible with the true distribution stem:[y]. Our predicted probability distribution stem:[\hat{y}] should be such that it should minimize the KL divergence.

[stem]
++++
\min_{\hat{y}} D_{KL}(y \parallel \hat{y}) = \min_{\hat{y}} \left( - \sum_i y_i \log (\hat{y}_i) + \sum_i y_i \log (y_i) \right) = \min_{\hat{y}} \left( - \sum_i y_i \log (\hat{y}_i) \right)
++++

As the second term is independent of stem:[\hat{y}], it is a constant with respect to stem:[\hat{y}]. We know this is cross-entropy.

[stem]
++++
CE = - \sum_i y_i \log (\hat{y}_i)
++++

Minimizing KL divergence is the same as minimizing CE. Minimizing CE makes the distributions stem:[y] and stem:[\hat{y}] similar, so it is very reasonable to use cross-entropy as the objective function in classification models.

We can see that as CE gets smaller, the predicted probability stem:[\hat{y}] gets exponentially closer to the actual probability stem:[y]. For classes stem:[i=0,1,2]

* For a data point, let's say the actual class probability distribution is stem:[y=[1,0,0\]] (one-hot encoded).
* The predicted class probability distribution is stem:[\hat{y} = [0.7, 0.1, 0.2\]].
* stem:[CE = - \sum_i y_i \log (\hat{y}_i)]

Jensen's inequality of stem:[\log(X)] is

[stem]
++++
\begin{align*}
\log (\mathbb{E}[\hat{y}]) & \geq \mathbb{E}[ \log (\hat{y})] \\
\log (\sum_i y_i \hat{y}_i) & \geq \sum_i y_i \log(\hat{y}_i) \\
\log (\sum_i y_i \hat{y}_i) & \geq -CE \\
\sum_i y_i \hat{y}_i & = 1*0.7 + 0*0.1 + 0*0.2 = 0.7 = P({y=\hat{y}}) \\
\log (P({y=\hat{y}})) & \geq -CE  \\
\end{align*}
++++

stem:[\sum_i y_i \hat{y}_i] denotes the probability that stem:[y] and stem:[\hat{y}] are the same. Making the CE smaller, increases the lower bound, and therefore increases the probability that stem:[\hat{y}] will be stem:[y].

[stem]
++++
P({y=\hat{y}}) \geq e^{-CE} 
++++

As we make CE (the CE formula is as above) smaller, the probability that stem:[\hat{y}] becomes stem:[y] increases exponentially.