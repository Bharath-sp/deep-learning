= Properties of ANN =
:doctype: book
:stem: latexmath
:eqnums:
:toc:

== Linearity vs Non-linearity of ANN ==
A single layer perceptron (SLP) is a neural network consisting only of an input layer and an output layer. There is no hidden layer. SLP can learn only linear functions. For classification, only linear separation is possible. For regression, only linear regression is possible.

A multi-layer perceptron (MLP) is a neural network with hidden layers. It can learn linear and non-linear functions.

image::.\images\slp_vs_mlp.png[align='center', 500, 400]

With a multi-layer perceptron, non-linear problems cannot be solved if a linear activation function is used in the hidden layers. This is because multi-layer neural networks with linear activation functions in the hidden layers behave like single layer neural networks. This follows from the fact that the composition of successive linear transformations is itself a linear transformation, i.e. linear function of a linear function is basically a linear function.

*Proof:*

The output of a multi-layer NN is

[stem]
++++
\begin{align*}
\hat{y} & = f_o(f_h(\mathbf{X} \cdot \mathbf{W}_h + \mathbf{b}_h) \cdot \mathbf{W}_o + \mathbf{b}_o) \\
\hat{y} & = f_o((\mathbf{X} \cdot \mathbf{W}_h + \mathbf{b}_h) \cdot \mathbf{W}_o + \mathbf{b}_o) && \text{if } f_h(\mathbf{x})=\mathbf{x}\\
\hat{y} & = f_o(\mathbf{X} \cdot \mathbf{W}_h \cdot \mathbf{W}_o + \mathbf{b}_h \cdot \mathbf{W}_o + \mathbf{b}_o) \\
\hat{y} & = f_o(\mathbf{X} \cdot \mathbf{W} + \mathbf{b}) && \mathbf{W} = \mathbf{W}_h \cdot \mathbf{W}_o, \mathbf{b} = \mathbf{b}_h \cdot \mathbf{W}_o + \mathbf{b}_o   \\ 
\end{align*}
++++

This equation is equivalent to the output of a single-layer NN. So if we use linear activation function in the hidden layers, we loose the benefit of using the hidden layers.

== Vanishing Gradient ==
If hidden layers use non-linear activation functions, such as sigmoid or hyperbolic tangent, the gradients of loss function may not propagate properly to lower layers. This is called the vanishing gradient problem. The maximum value of the derivative of the sigmoid function is 0.25. During backpropagation, the gradient is repeatedly multiplied by values less than 0.25, which can cause it to decrease exponentially or vanish.

Say we have a neural network with two hidden layers as below. Each parameter is updated through gradient descent method.

[stem]
++++
w \leftarrow w - \alpha \frac{\partial L}{\partial w}
++++

where stem:[\frac{\partial L}{\partial w}] is the gradient of the loss function. By backpropagation, the loss in the output layer is propagated to the hidden layers. And the parameters in the hidden layers are updated using the formula below

image::.\images\vanishing_gradient.png[align='center', 500, 400]

* We see that the second term on RHS is multiplied by the derivative of stem:[f_{h,2}], which is the activation function of the second hidden layer. The maximum value of the derivative of the sigmoid function is 0.25. If stem:[f_{h,2}] is sigmoid, its maximum value is 0.25, and let's say on average it is 0.1.

* In the first layer, the derivative of stem:[f_{h,1}] is multiplied once more. If stem:[f_{h,1}] is also sigmoid, its maximum value is 0.25, and let's say on average it is 0.1.

If there were 5 hidden layers in this network, in the bottom layer, 0.1 is multiplied 5 times. As there are more hidden layers, the gradients become very small. If the second term is very small, the update of the parameter will not be proper.

To alleviate this problem, ReLU is widely used in hidden layers. ReLU is a non-linear function that combines two linear functions. The gradient of ReLU is either 0 or 1. If it is 1, the gradient propagates well to the lower layers. If it is 0, the gradient will not propagate. However, with ReLU, the output value of some neurons in the hidden layer will become 0, causing some of the dropout and regularization effects.

In the worst case, if the output of all the neurons in the hidden layer becomes 0, the gradient cannot propagate to lower layers. This is called the *dying ReLU* problem. This problem may appear if the learning rate stem:[\alpha] is large, or the bias is large and negative. This is because they make the weights negative and the output of ReLU 0.

[stem]
++++
\mathbf{W}_{h,1} \leftarrow \mathbf{W}_{h,1} - \alpha \, \_\_ \, f_o' \, f_{h,2}' \, f_{h,1}'
++++

Say stem:[f_{h,1}, f_{h,2}] are ReLU, their derivative is either 0 or 1. And when the stem:[\alpha \, \_\_ \, f_o'] is also positive and large, the term stem:[\alpha \, \_\_ \, f_o' \, f_{h,2}' \, f_{h,1}'] will be greater than current stem:[\mathbf{W}_{h,1}], thus the updated stem:[\mathbf{W}_{h,1}] will be negative. If the bias stem:[\mathbf{b}_{h,1}] is also large negative and the elements of stem:[\mathbf{X}] are all positive, then the input to ReLU  stem:[(\mathbf{X \cdot \mathbf{W}_{h,1} + \mathbf{b}_{h,1}})] will be negative. Hence the outputs from this hidden layer will be all 0.

To alleviate this problem, we can try lowering the learning rate and preventing the bias from becoming large. We can also try using the leaky ReLU or softplus activation function.

image::.\images\activations_derivatives.png[align='center', 600, 500]

== Network Architecture ==
There are different ways to construct feed forward neural networks.

* The network can be sparse, with not all possible connections within a layer being present (left figure). stem:[z_1] has connections only with few input variables.
* Skip connects may be added (stem:[x_1] is directly connected to stem:[y_1] skipping the hidden layers, in the right figure).

image::.\images\neural_network_04.png[align='center', 700, 500]

These networks are called as feed forward neural networks (FFNN) because there are no loops, input is fed only in one direction (forward). And here the outputs, intermediate layers are *deterministic* functions of inputs.

== Adaptable Basis Functions ==
Illustration of the capability of a multilayer perceptron to approximate four different functions: stem:[f(x) = x^2], stem:[f(x) = sin(x)], stem:[f(x) = |x|], and stem:[f(x) = H(x)], where stem:[H(x)] is a Heaviside step function.

In each case, stem:[N = 50] data points, shown as blue dots in the graph below, have been sampled uniformly in stem:[x] over the interval stem:[(-1, 1)] and the corresponding values of stem:[f(x)] evaluated. These data points are then used to train a two-layer network having 3 hidden units with `tanh` activation functions and a linear output unit.

image::.\images\sample_neural_network.png[align='center', 600, 400]

In total, there are 10 parameters, 6 weight and 4 bias parameters. The resulting network function stem:[y] is shown by the red curves, and the basis functions learnt by three hidden units stem:[(z_1, z_2, z_3)] are shown by the three dashed curves.

image::.\images\nn_approximation.png[align='center', 500, 700]

With the given simple network, we are able to learn different functions (by training on different data samples). This is achievable because the basis functions stem:[(z_1, z_2, z_3)] are adaptive, and learnt based on the data.

== Weight-space Symmetries ==
One property of feed-forward networks is that multiple distinct choices for the weight vector stem:[\mathbf{w}] can all give rise to the same mapping function from inputs to outputs. 

Consider a two-layer network as below with stem:[M=3] hidden units having 'tanh' activation functions and full connectivity in both layers.

image::.\images\sample_neural_network.png[align='center', 600, 400]

If we change the sign of all of the weights and the bias feeding into a particular hidden unit, then, for a given input stem:[x], the sign of the activation of the hidden unit will be reversed, because 'tanh' is an odd function, so that stem:[tanh(-a) = - tanh(a)]. say we change the sign of stem:[w_0, w_1]. Then,

[stem]
++++
z_1 = tanh(-w_0 -w_1x) = - tanh(w_0 + w_1x)
++++

This transformation can be exactly compensated by changing the sign of all of the weights leading out of that hidden unit. We change the sign of stem:[w_6]. Then the resulting stem:[y] remains the same. Thus, by changing the signs of a particular group of weights (and a bias), the input-output mapping function represented by the network is unchanged. So we have found two different weight vectors that give rise to the same mapping function.

For stem:[M] hidden units, there will be stem:[M] such sign-flip symmetries, and thus any given weight vector will be one of stem:[2^M] equivalent weight vectors.

Similarly, imagine that we interchange the values of all of the weights (and the bias) leading both into and out of a particular hidden unit with the corresponding
values of the weights (and bias) associated with a different hidden unit. Let's define,

[stem]
++++
\begin{align*}
z_1 & = tanh(w_3x + w_2) \\
z_2 & = tanh(w_1x + w_0) \\
z_3 & = tanh(w_5x + w_4) \\
y & = w_7 z_1  + w_6 z_2  + w_8 z_3 + w_9
\end{align*}
++++

stem:[y] remains the same. Again, this clearly leaves the network input-output mapping function unchanged, but it corresponds to a different choice of weight vector. This amounts to arranging stem:[M] distint objects. For stem:[M] hidden units, any given weight vector will belong to a set of stem:[M!] equivalent weight vectors associated with this interchange symmetry.

The network will therefore have an overall weight-space symmetry factor of stem:[M!2^M]. For networks with more than two layers of weights, the total level of symmetry will be given by the product of such factors, one for each layer of hidden units. 

NOTE: the existence of these symmetries is not a particular property of the 'tanh' function but applies to a wide range of activation functions.

Here we are learning the parameters in a high-dimensional space. In high-dimensional optimization, we get a lot of equivalent or similar local minima. So different sets of parameters gives us the same model performance. Even if we keep training data, number of layers, activations and other configurations the same and learn from a different random initialization, we may end up learning different basis functions.